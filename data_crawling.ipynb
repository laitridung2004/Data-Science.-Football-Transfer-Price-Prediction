{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "089d30aa",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center;\"> DATA SCIENCE CAPSTONE PROJECT </div>\n",
    "\n",
    "## <div style=\"text-align: center;\"> DOMINATE THE TRANSFER MARKET: FOOTBALL PLAYER PRICE PREDICTION </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2f3c2e",
   "metadata": {},
   "source": [
    "### <div style=\"text-align: center;\"> DATA CRAWLING SECTION </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d21b1",
   "metadata": {},
   "source": [
    "#### 0. How did we create the final Dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_PLAYERS_LINK = \"C:/Users/Admin/OneDrive - Hanoi University of Science and Technology/Desktop/players_link.csv\"\n",
    "PATH_TO_PLAYERS_DATA = \"C:/Users/Admin/OneDrive - Hanoi University of Science and Technology/Desktop/players_data.csv\"\n",
    "PATH_TO_NOT_GK_PLAYERS_DATA = \"C:/Users/Admin/OneDrive - Hanoi University of Science and Technology/Desktop/not_gk_players_data.csv\"\n",
    "PATH_TO_GK_PLAYERS_DATA = \"C:/Users/Admin/OneDrive - Hanoi University of Science and Technology/Desktop/gk_players_data.csv\"\n",
    "PATH_TO_NOT_GK_PLAYERS_LINK = \"C:/Users/Admin/OneDrive - Hanoi University of Science and Technology/Desktop/not_gk_players_link.csv\"\n",
    "PATH_TO_GK_PLAYERS_LINK = \"C:/Users/Admin/OneDrive - Hanoi University of Science and Technology/Desktop/gk_players_link.csv\"\n",
    "PATH_TO_NOT_GK_PLAYERS_STAT = \"C:/Users/Admin/OneDrive - Hanoi University of Science and Technology/Desktop/not_gk_players_stat.csv\"\n",
    "PATH_TO_GK_PLAYERS_STAT = \"C:/Users/Admin/OneDrive - Hanoi University of Science and Technology/Desktop/gk_players_stat.csv\"\n",
    "PATH_TO_NOT_GK_PLAYERS = 'C:/Users/Admin/OneDrive - Hanoi University of Science and Technology/Desktop/not_gk_players.csv'\n",
    "PATH_TO_GK_PLAYERS = 'C:/Users/Admin/OneDrive - Hanoi University of Science and Technology/Desktop/gk_players.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeff593",
   "metadata": {},
   "source": [
    "Listing vital features for valuing players is our priority first by approaching some related papers. After researching topics and surfing the web page Transfermarkt, we noticed that to get all the features needed, we have to reach two different addresses linked to each players, which is called **(general) data** and **(performance) statistics**. While the former directs us quite a general information about players such as the name, club, age, ... which could be effortlessly remembered for all football viewers, the latter focus on players' detailed analysising statistics in the match that is more about the expertise. To approach both tasks, we must get the players'id which is their distinctive features in Transfermarkt. The only consideration is that in the **(performance) statistics** work, the **goalkeeper** and **other positions** have two different set of evaluation, requiring us split the collected id into two new files. After having all the needed feature obtained, our last job is to concatenate all the Dataframe together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c86f2",
   "metadata": {},
   "source": [
    "#### 1. IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T05:20:01.671284Z",
     "start_time": "2023-12-17T05:20:01.578104Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests, time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68373221",
   "metadata": {},
   "source": [
    "#### 2. PLAYERS' LINK SCRAPING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1cbb02",
   "metadata": {},
   "source": [
    "Getting the browser's user agent for sending requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "607ddc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\", \"accept-language\": \"en-US,en;q=0.9\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019eeb1c",
   "metadata": {},
   "source": [
    "Getting a list of club needed scraping (top 5 football leagues in the world are considered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07329dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = pd.DataFrame([{'CountryID': 189, 'Country': 'England'},\n",
    "        {'CountryID': 40, 'Country': 'Germany'},\n",
    "        {'CountryID': 75, 'Country': 'Italy'},\n",
    "        {'CountryID': 50, 'Country': 'France'},\n",
    "        {'CountryID': 157, 'Country': 'Spain'}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fc8678",
   "metadata": {},
   "source": [
    "Getting leagues' name and url (top 2 football leagues each countries are considered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8be91f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "league_name, league_url = [], []\n",
    "for i in range(len(country)):\n",
    "    url = f\"https://www.transfermarkt.com/wettbewerbe/national/wettbewerbe/{country.loc[i,'CountryID']}\"\n",
    "    page = requests.get(url, headers = headers)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    for j in range(1, 3):\n",
    "        league_span = soup.select('.inline-table a')[j]\n",
    "        league_name.append(league_span.get('title'))\n",
    "        league_url.append('https://www.transfermarkt.com' + league_span.get('href') + '/plus/?saison_id=')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdd1765",
   "metadata": {},
   "source": [
    "Getting Transfermarkt's href of all the players in these leagues (6 seasons are considered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad2e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Players_Link = []\n",
    "for league, url in zip(league_name, league_url):\n",
    "    for season in range(2018, 2024):\n",
    "        page = requests.get(url + str(season), headers = headers)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        club_urls = [link.get(\"href\") for link in soup.select(\"#yw1 .no-border-links a:nth-child(1)\")]\n",
    "        for club_url in club_urls:\n",
    "            club_id = club_url.split(\"/\")[-3]\n",
    "            club_page = requests.get(\"https://www.transfermarkt.com\" + club_url, headers=headers)                  \n",
    "            soup2 = BeautifulSoup(club_page.content, \"html.parser\")\n",
    "\n",
    "            players_list = soup2.select(\".inline-table .hauptlink > a\")\n",
    "            All_Players_Link.extend(p.get('href') for p in players_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ae550",
   "metadata": {},
   "source": [
    "Dropping duplicates and store them to a csv file named \"players_link\" serving as players web's adress to scrape their datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9099959",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(All_Players_Link).drop_duplicates().to_csv(PATH_TO_PLAYERS_LINK, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb827cb6",
   "metadata": {},
   "source": [
    "#### 3. PLAYERS' (GENERAL) INFORMATION SCRAPING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4128c776",
   "metadata": {},
   "source": [
    "Defining function *scraping_player_data* to scrape players' (general) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d9bed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_players_data(url, players_datas):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\", \"accept-language\": \"en-US,en;q=0.9\"}\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    data = {}\n",
    "\n",
    "    pattern = r\"/(\\d+)$\"\n",
    "    match = re.search(pattern, url)\n",
    "    player_id = match.group(1)\n",
    "    data[\"player_id\"] = player_id\n",
    "\n",
    "    try:\n",
    "        name = soup.select_one('h1[class = \"data-header__headline-wrapper\"]').text.split(\"\\n\")[-1].strip()\n",
    "    except AttributeError:\n",
    "        name = None\n",
    "    data[\"name\"] = name\n",
    "\n",
    "    try:\n",
    "        player_club = soup.select_one(\"span[class = 'data-header__club']\").text.strip()\n",
    "    except AttributeError:\n",
    "        player_club = None\n",
    "    except ValueError:\n",
    "        player_club = None\n",
    "    except IndexError:\n",
    "        player_club = None\n",
    "    data[\"player_club\"] = player_club\n",
    "\n",
    "    try:\n",
    "        age = float(soup.select_one('li[class=\"data-header__label\"]').text.split(\"\\n\")[-2].split()[-1].strip(\"()\"))\n",
    "    except AttributeError:\n",
    "        age = None\n",
    "    except ValueError:\n",
    "        age = None\n",
    "    except IndexError:\n",
    "        age = None\n",
    "    data[\"age\"] = age\n",
    "\n",
    "    try:\n",
    "        position = soup.find('dd', class_='detail-position__position').text\n",
    "    except AttributeError:\n",
    "        position = None\n",
    "    except ValueError:\n",
    "        position = None\n",
    "    except IndexError:\n",
    "        position = None\n",
    "    data[\"position\"] = position\n",
    "\n",
    "    try:\n",
    "        market_value = soup.select_one('a[class=\"data-header__market-value-wrapper\"]').text.split(\" \")[0].replace('€', '')\n",
    "        if \"m\" in market_value:\n",
    "            market_value = market_value.replace(\"m\", \"\")\n",
    "            market_value = float(market_value)*1000\n",
    "        elif \"k\" in market_value:\n",
    "            market_value = market_value.replace(\"k\", \"\")\n",
    "            market_value = float(market_value)\n",
    "    except AttributeError:\n",
    "        market_value = None\n",
    "    except ValueError:\n",
    "        market_value = None\n",
    "    except IndexError:\n",
    "        market_value = None\n",
    "    data[\"market_value\"] = market_value\n",
    "    \n",
    "    try:\n",
    "        nationality = soup.find('span', itemprop = \"nationality\").text.strip()\n",
    "    except AttributeError:\n",
    "        nationality = None\n",
    "    except ValueError:\n",
    "        nationality = None\n",
    "    except IndexError:\n",
    "        nationality = None\n",
    "    data[\"nationality\"] = nationality\n",
    "\n",
    "    try:\n",
    "        player_height = float(re.search(\"Height:.*?([0-9].*?)\\n\", soup.text, re.DOTALL).group(1).strip().split(\" \")[0].replace(\",\", \".\"))\n",
    "    except AttributeError:\n",
    "        player_height = None\n",
    "    except ValueError:\n",
    "        player_height = None\n",
    "    except IndexError:\n",
    "        player_height = None\n",
    "    data[\"player_height\"] = player_height\n",
    "\n",
    "    try:\n",
    "        player_agent = re.search(\"Agent:.*?([A-z].*?)\\n\", soup.text, re.DOTALL).group(1).strip()\n",
    "    except AttributeError:\n",
    "        player_agent = None\n",
    "    except ValueError:\n",
    "        player_agent = None\n",
    "    except IndexError:\n",
    "        player_agent = None\n",
    "    data[\"player_agent\"] = player_agent\n",
    "\n",
    "    try:\n",
    "        strong_foot = soup.select('span[class = \"info-table__content info-table__content--bold\"]')[6].text\n",
    "    except AttributeError:\n",
    "        strong_foot = None\n",
    "    except ValueError:\n",
    "        strong_foot = None\n",
    "    except IndexError:\n",
    "        strong_foot = None\n",
    "    data[\"strong_foot\"] = strong_foot\n",
    "\n",
    "    try:\n",
    "        contract_value_time = float(re.search(\"Contract expires: (.*)\", soup.text).group(1).split()[-1])\n",
    "    except AttributeError:\n",
    "        contract_value_time = None\n",
    "    except ValueError:\n",
    "        contract_value_time = None\n",
    "    data[\"contract_value_time\"] = contract_value_time\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f354ad",
   "metadata": {},
   "source": [
    "Defining column *data_column* corresponding the data scraped by the function *scraping_players_data*. This step is to create the Dataframe *players_data*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b94e67",
   "metadata": {},
   "source": [
    "Note: Dataframe *players_data* is not only used as a partial Dataframe (which would then be concatenated with other Dataframe to create a final Dataframe \n",
    "for predicting model) but also called to determine players' position. We will then use this position to create two new Dataframes for evaluating players' performance\n",
    "as the estimation criteria is different for **goalkeeper** and **other positions** (based on Transfermarkt's data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f994cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_column = [\"player_id\", \"name\", \"player_club\", \"age\", \"position\", \"market_value\", \"nationality\", \"player_height\", \"player_agent\", \"strong_foot\", \"contract_value_time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3c72bc",
   "metadata": {},
   "source": [
    "Creating Dataframe *players_data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd2e01b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_data = pd.DataFrame(columns = data_column).astype(str)\n",
    "hyperlink = 'https://www.transfermarkt.com' + pd.read_csv(PATH_TO_PLAYERS_LINK)\n",
    "\n",
    "for i in range(len(hyperlink)):\n",
    "    single_player_data = scraping_players_data(hyperlink.loc[i, \"0\"], players_data)\n",
    "    players_data = pd.concat([players_data, pd.DataFrame([single_player_data])], ignore_index=True)\n",
    "\n",
    "pd.DataFrame(players_data).to_csv(PATH_TO_PLAYERS_DATA, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70326d4",
   "metadata": {},
   "source": [
    "#### 4. PLAYERS' (PERFORMANCE) STATISTICS SCRAPING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f030ac56",
   "metadata": {},
   "source": [
    "Creating two new csv basis for **goalkeeper** and **other positions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e05263",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_data = pd.read_csv(PATH_TO_PLAYERS_DATA)\n",
    "\n",
    "not_gk_players_data = players_data[players_data['position'] != 'Goalkeeper']\n",
    "gk_players_data = players_data[players_data['position'] == 'Goalkeeper']\n",
    "\n",
    "not_gk_players_data.to_csv(PATH_TO_NOT_GK_PLAYERS_DATA, index=False)\n",
    "gk_players_data.to_csv(PATH_TO_GK_PLAYERS_DATA, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f3ac9b",
   "metadata": {},
   "source": [
    "Splitting the csv file *players_link* into two new csv file for **goalkeeper** link and **other positions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c53e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "gk_players_data = pd.read_csv(PATH_TO_GK_PLAYERS_DATA)\n",
    "players_link = pd.read_csv(PATH_TO_PLAYERS_LINK)\n",
    "\n",
    "gk_ids = gk_players_data['player_id'].astype(str)\n",
    "\n",
    "not_gk_players_link = players_link[~players_link.iloc[:, 0].str.split('/').str[-1].isin(gk_ids)]\n",
    "gk_players_link = players_link[players_link.iloc[:, 0].str.split('/').str[-1].isin(gk_ids)]\n",
    "\n",
    "not_gk_players_link.to_csv(PATH_TO_NOT_GK_PLAYERS_LINK, index=False)\n",
    "gk_players_link.to_csv(PATH_TO_GK_PLAYERS_LINK, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b49bdaf",
   "metadata": {},
   "source": [
    "Defining function *scraping_not_gk_stat* to scrape **other players**' (performance) statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "509d5a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_not_gk_stat(url, name):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36\", \"accept-language\": \"en-US,en;q=0.9\"}\n",
    "\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    not_gk_stat = {}\n",
    "\n",
    "    try:\n",
    "        appearances = soup.find_all(\"td\", {\"class\": \"zentriert\"})[1].text\n",
    "        if (appearances == \"-\"):\n",
    "            appearances = 0\n",
    "        else:\n",
    "            appearances = float(appearances)\n",
    "    except ValueError:\n",
    "        appearances = None\n",
    "    except AttributeError:\n",
    "        appearances = None\n",
    "    except IndexError:\n",
    "        appearances = None\n",
    "    not_gk_stat[\"appearances\"] = appearances\n",
    "\n",
    "    try:\n",
    "        PPG = soup.find_all(\"td\", {\"class\": \"zentriert\"})[2].text\n",
    "        if (PPG == \"-\"):\n",
    "            PPG = 0\n",
    "        else:\n",
    "            PPG = float(PPG)\n",
    "    except ValueError:\n",
    "        PPG = None\n",
    "    except AttributeError:\n",
    "        PPG = None\n",
    "    except IndexError:\n",
    "        PPG = None\n",
    "    not_gk_stat[\"PPG\"] = PPG \n",
    "\n",
    "    try:\n",
    "        goals = soup.find_all(\"td\", {\"class\": \"zentriert\"})[3].text\n",
    "        if (goals == \"-\"):\n",
    "            goals = 0\n",
    "        else:\n",
    "            goals = float(goals)\n",
    "    except ValueError:\n",
    "        goals = None\n",
    "    except AttributeError:\n",
    "        goals = None\n",
    "    except IndexError:\n",
    "        goals = None\n",
    "    not_gk_stat[\"goals\"] = goals\n",
    "\n",
    "    try:\n",
    "        assists = soup.find_all(\"td\", {\"class\": \"zentriert\"})[4].text\n",
    "        if (assists == \"-\"):\n",
    "            assists = 0\n",
    "        else:\n",
    "            assists = float(assists)\n",
    "    except ValueError:\n",
    "        assists = None\n",
    "    except AttributeError:\n",
    "        assists = None\n",
    "    except IndexError:\n",
    "        assists = None\n",
    "    not_gk_stat[\"assists\"] = assists\n",
    "\n",
    "    try:\n",
    "        own_goals = soup.find_all(\"td\", {\"class\": \"zentriert\"})[5].text\n",
    "        if (own_goals == \"-\"):\n",
    "            own_goals = 0\n",
    "        else:\n",
    "            own_goals = float(own_goals)\n",
    "    except ValueError:\n",
    "        own_goals = None\n",
    "    except AttributeError:\n",
    "        own_goals = None\n",
    "    except IndexError:\n",
    "        own_goals = None\n",
    "    not_gk_stat[\"own_goals\"] = own_goals \n",
    "\n",
    "    try:\n",
    "        substitutions_on = soup.find_all(\"td\", {\"class\": \"zentriert\"})[6].text\n",
    "        if (substitutions_on == \"-\"):\n",
    "            substitutions_on = 0\n",
    "        else:\n",
    "            substitutions_on = float(substitutions_on)\n",
    "    except ValueError:\n",
    "        substitutions_on = None\n",
    "    except AttributeError:\n",
    "        substitutions_on = None\n",
    "    except IndexError:\n",
    "        substitutions_on = None\n",
    "    not_gk_stat[\"substitutions_on\"] = substitutions_on \n",
    "\n",
    "    try:\n",
    "        substitutions_off = soup.find_all(\"td\", {\"class\": \"zentriert\"})[7].text\n",
    "        if (substitutions_off == \"-\"):\n",
    "            substitutions_off = 0\n",
    "        else:\n",
    "            substitutions_off = float(substitutions_off)\n",
    "    except ValueError:\n",
    "        substitutions_off = None\n",
    "    except AttributeError:\n",
    "        substitutions_off = None\n",
    "    except IndexError:\n",
    "        substitutions_off = None\n",
    "    not_gk_stat[\"substitutions_off\"] = substitutions_off\n",
    "    \n",
    "    try:\n",
    "        yellow_cards = soup.find_all(\"td\", {\"class\": \"zentriert\"})[8].text\n",
    "        if (yellow_cards == \"-\"):\n",
    "            yellow_cards = 0\n",
    "        else:\n",
    "            yellow_cards = float(yellow_cards)\n",
    "    except ValueError:\n",
    "        yellow_cards = None\n",
    "    except AttributeError:\n",
    "        yellow_cards = None\n",
    "    except IndexError:\n",
    "        yellow_cards = None\n",
    "    not_gk_stat[\"yellow_cards\"] = yellow_cards\n",
    "\n",
    "    try:\n",
    "        second_yellow_cards = soup.find_all(\"td\", {\"class\": \"zentriert\"})[9].text\n",
    "        if (second_yellow_cards == \"-\"):\n",
    "            second_yellow_cards = 0\n",
    "        else:\n",
    "            second_yellow_cards = float(second_yellow_cards)\n",
    "    except ValueError:\n",
    "        second_yellow_cards = None\n",
    "    except AttributeError:\n",
    "        second_yellow_cards = None\n",
    "    except IndexError:\n",
    "        second_yellow_cards = None\n",
    "    not_gk_stat[\"second_yellow_cards\"] = second_yellow_cards\n",
    "\n",
    "    try:\n",
    "        red_cards = soup.find_all(\"td\", {\"class\": \"zentriert\"})[10].text\n",
    "        if (red_cards == \"-\"):\n",
    "            red_cards = 0\n",
    "        else:\n",
    "            red_cards = float(red_cards)\n",
    "    except ValueError:\n",
    "        red_cards = None\n",
    "    except AttributeError:\n",
    "        red_cards = None\n",
    "    except IndexError:\n",
    "        red_cards = None\n",
    "    not_gk_stat[\"red_cards\"] = red_cards\n",
    "\n",
    "    try:\n",
    "        penalty_goals = soup.find_all(\"td\", {\"class\": \"zentriert\"})[11].text\n",
    "        if (penalty_goals == \"-\"):\n",
    "            penalty_goals = 0\n",
    "        else:\n",
    "            penalty_goals = float(penalty_goals)\n",
    "    except ValueError:\n",
    "        penalty_goals = None\n",
    "    except AttributeError:\n",
    "        penalty_goals = None\n",
    "    except IndexError:\n",
    "        penalty_goals = None\n",
    "    not_gk_stat[\"penalty_goals\"] = penalty_goals\n",
    "\n",
    "    try:\n",
    "        minutes_per_goal = soup.find_all(\"td\", {\"class\": \"rechts\"})[1].text.split(\"'\")[0]\n",
    "        if (minutes_per_goal == \"-\"):\n",
    "            minutes_per_goal = 0\n",
    "        else:\n",
    "            minutes_per_goal = float(minutes_per_goal)\n",
    "    except ValueError:\n",
    "        minutes_per_goal = None\n",
    "    except AttributeError:\n",
    "        minutes_per_goal = None\n",
    "    except IndexError:\n",
    "        minutes_per_goal = None\n",
    "    not_gk_stat[\"minutes_per_goal\"] = minutes_per_goal\n",
    "\n",
    "    try:\n",
    "        minutes_played = soup.find_all(\"td\", {\"class\": \"rechts\"})[2].text.split(\"'\")[0]\n",
    "        if (minutes_played == \"-\"):\n",
    "            minutes_played = 0\n",
    "        else:\n",
    "            minutes_played = float(minutes_played)\n",
    "    except ValueError:\n",
    "        minutes_played = None\n",
    "    except AttributeError:\n",
    "        minutes_played = None\n",
    "    except IndexError:\n",
    "        minutes_played = None\n",
    "    not_gk_stat[\"minutes_played\"] = minutes_played\n",
    "\n",
    "    return not_gk_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a7d0a",
   "metadata": {},
   "source": [
    "Defining column *not_gk_stat_column* corresponding the data scraped by the function *scraping_not_gk_stat*. This step is to create the Dataframe *not_gk_players_stat*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe30fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_gk_stat_column = [\"appearances\", \"PPG\", \"goals\", \"assists\", \"own_goals\", \"substitutions_on\", \"substitutions_off\", \"yellow_cards\", \"second_yellow_cards\", \"red_cards\", \"penalty_goals\", \"minutes_per_goal\", \"minutes_played\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3751d87",
   "metadata": {},
   "source": [
    "Creating Dataframe *not_gk_players_stat*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c55a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_gk_players_stat = pd.DataFrame(columns = not_gk_stat_column).astype(str)\n",
    "not_gk_players_link = 'https://www.transfermarkt.com' + pd.read_csv(PATH_TO_NOT_GK_PLAYERS_LINK)\n",
    "\n",
    "for i in range(len(not_gk_players_link)):\n",
    "    id = not_gk_players_link.loc[i, \"0\"].split('spieler/')[-1]\n",
    "    name = not_gk_players_link.loc[i, \"0\"].split('com/')[-1].split('/profil')[0].replace(' ', '-')\n",
    "    not_gk_players_hyperlink = f\"https://www.transfermarkt.com/{name}/leistungsdatendetails/spieler/{id}/plus/1?saison=2024&verein=&liga=&wettbewerb=&pos=&trainer_id=\"\n",
    "    single_not_gk_player_stat = scraping_not_gk_stat(not_gk_players_hyperlink, name)\n",
    "    not_gk_players_stat = pd.concat([not_gk_players_stat, pd.DataFrame([single_not_gk_player_stat])], ignore_index=True)\n",
    "\n",
    "pd.DataFrame(not_gk_players_stat).to_csv(PATH_TO_NOT_GK_PLAYERS_STAT, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a70d2",
   "metadata": {},
   "source": [
    "Defining function *scraping_gk_stat* to scrape **goalkeeper**' (performance) statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e191b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_gk_stat(url, name):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\",\n",
    "               \"accept-language\": \"en-US,en;q=0.9\"}\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    gk_stat = {}\n",
    "\n",
    "    try:\n",
    "        appearances = soup.find_all(\"td\", {\"class\": \"zentriert\"})[1].text\n",
    "        if (appearances == \"-\"):\n",
    "            appearances = 0\n",
    "        else:\n",
    "            appearances = float(appearances)\n",
    "    except ValueError:\n",
    "        appearances = None\n",
    "    except AttributeError:\n",
    "        appearances = None\n",
    "    except IndexError:\n",
    "        appearances = None\n",
    "    gk_stat[\"appearances\"] = appearances\n",
    "\n",
    "    try:\n",
    "        PPG = soup.find_all(\"td\", {\"class\": \"zentriert\"})[2].text\n",
    "        if (PPG == \"-\"):\n",
    "            PPG = 0\n",
    "        else:\n",
    "            PPG = float(PPG)\n",
    "    except ValueError:\n",
    "        PPG = None\n",
    "    except AttributeError:\n",
    "        PPG = None\n",
    "    except IndexError:\n",
    "        PPG = None\n",
    "    gk_stat[\"PPG\"] = PPG \n",
    "\n",
    "    try:\n",
    "        goals = soup.find_all(\"td\", {\"class\": \"zentriert\"})[3].text\n",
    "        if (goals == \"-\"):\n",
    "            goals = 0\n",
    "        else:\n",
    "            goals = float(goals)\n",
    "    except ValueError:\n",
    "        goals = None\n",
    "    except AttributeError:\n",
    "        goals = None\n",
    "    except IndexError:\n",
    "        goals = None\n",
    "    gk_stat[\"goals\"] = goals\n",
    "\n",
    "    try:\n",
    "        own_goals = soup.find_all(\"td\", {\"class\": \"zentriert\"})[4].text\n",
    "        if (own_goals == \"-\"):\n",
    "            own_goals = 0\n",
    "        else:\n",
    "            own_goals = float(own_goals)\n",
    "    except ValueError:\n",
    "        own_goals = None\n",
    "    except AttributeError:\n",
    "        own_goals = None\n",
    "    except IndexError:\n",
    "        own_goals = None\n",
    "    gk_stat[\"own_goals\"] = own_goals \n",
    "\n",
    "    try:\n",
    "        substitutions_on = soup.find_all(\"td\", {\"class\": \"zentriert\"})[5].text\n",
    "        if (substitutions_on == \"-\"):\n",
    "            substitutions_on = 0\n",
    "        else:\n",
    "            substitutions_on = float(substitutions_on)\n",
    "    except ValueError:\n",
    "        substitutions_on = None\n",
    "    except AttributeError:\n",
    "        substitutions_on = None\n",
    "    except IndexError:\n",
    "        substitutions_on = None\n",
    "    gk_stat[\"substitutions_on\"] = substitutions_on \n",
    "\n",
    "    try:\n",
    "        substitutions_off = soup.find_all(\"td\", {\"class\": \"zentriert\"})[6].text\n",
    "        if (substitutions_off == \"-\"):\n",
    "            substitutions_off = 0\n",
    "        else:\n",
    "            substitutions_off = float(substitutions_off)\n",
    "    except ValueError:\n",
    "        substitutions_off = None\n",
    "    except AttributeError:\n",
    "        substitutions_off = None\n",
    "    except IndexError:\n",
    "        substitutions_off = None\n",
    "    gk_stat[\"substitutions_off\"] = substitutions_off\n",
    "    \n",
    "    try:\n",
    "        yellow_cards = soup.find_all(\"td\", {\"class\": \"zentriert\"})[7].text\n",
    "        if (yellow_cards == \"-\"):\n",
    "            yellow_cards = 0\n",
    "        else:\n",
    "            yellow_cards = float(yellow_cards)\n",
    "    except ValueError:\n",
    "        yellow_cards = None\n",
    "    except AttributeError:\n",
    "        yellow_cards = None\n",
    "    except IndexError:\n",
    "        yellow_cards = None\n",
    "    gk_stat[\"yellow_cards\"] = yellow_cards\n",
    "\n",
    "    try:\n",
    "        second_yellow_cards = soup.find_all(\"td\", {\"class\": \"zentriert\"})[8].text\n",
    "        if (second_yellow_cards == \"-\"):\n",
    "            second_yellow_cards = 0\n",
    "        else:\n",
    "            second_yellow_cards = float(second_yellow_cards)\n",
    "    except ValueError:\n",
    "        second_yellow_cards = None\n",
    "    except AttributeError:\n",
    "        second_yellow_cards = None\n",
    "    except IndexError:\n",
    "        second_yellow_cards = None\n",
    "    gk_stat[\"second_yellow_cards\"] = second_yellow_cards\n",
    "\n",
    "    try:\n",
    "        red_cards = soup.find_all(\"td\", {\"class\": \"zentriert\"})[9].text\n",
    "        if (red_cards == \"-\"):\n",
    "            red_cards = 0\n",
    "        else:\n",
    "            red_cards = float(red_cards)\n",
    "    except ValueError:\n",
    "        red_cards = None\n",
    "    except AttributeError:\n",
    "        red_cards = None\n",
    "    except IndexError:\n",
    "        red_cards = None\n",
    "    gk_stat[\"red_cards\"] = red_cards\n",
    "\n",
    "    try:\n",
    "        goals_conceded = soup.find_all(\"td\", {\"class\": \"zentriert\"})[10].text\n",
    "        if (goals_conceded == \"-\"):\n",
    "            goals_conceded = 0\n",
    "        else:\n",
    "            goals_conceded = float(goals_conceded)\n",
    "    except ValueError:\n",
    "        goals_conceded = None\n",
    "    except AttributeError:\n",
    "        goals_conceded = None\n",
    "    except IndexError:\n",
    "        goals_conceded = None\n",
    "    gk_stat[\"goals_conceded\"] = goals_conceded\n",
    "\n",
    "    try:\n",
    "        clean_sheet = soup.find_all(\"td\", {\"class\": \"zentriert\"})[11].text\n",
    "        if (clean_sheet == \"-\"):\n",
    "            clean_sheet = 0\n",
    "        else:\n",
    "            clean_sheet = float(clean_sheet)\n",
    "    except ValueError:\n",
    "        clean_sheet = None\n",
    "    except AttributeError:\n",
    "        clean_sheet = None\n",
    "    except IndexError:\n",
    "        clean_sheet = None\n",
    "    gk_stat[\"clean_sheet\"] = clean_sheet\n",
    "\n",
    "    try:\n",
    "        minutes_played = soup.find_all(\"td\", {\"class\": \"rechts\"})[1].text.split(\"'\")[0]\n",
    "        if (minutes_played == \"-\"):\n",
    "            minutes_played = 0\n",
    "        else:\n",
    "            minutes_played = float(minutes_played)\n",
    "    except ValueError:\n",
    "        minutes_played = None\n",
    "    except AttributeError:\n",
    "        minutes_played = None\n",
    "    except IndexError:\n",
    "        minutes_played = None\n",
    "    gk_stat[\"minutes_played\"] = minutes_played\n",
    "\n",
    "    return gk_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a1d069",
   "metadata": {},
   "source": [
    "Defining column *gk_stat_column* corresponding the data scraped by the function *scraping_not_gk_stat*. This step is to create the Dataframe *gk_players_stat*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8345c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gk_stat_column = [\"appearances\", \"PPG\", \"goals\", \"own_goals\", \"substitutions_on\", \"substitutions_off\", \"yellow_cards\", \"second_yellow_cards\", \"red_cards\", \"goals_conceded\", \"clean_sheet\", \"minutes_played\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61642e10",
   "metadata": {},
   "source": [
    "Creating Dataframe *gk_players_stat*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaac69d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gk_players_stat = pd.DataFrame(columns = gk_stat_column).astype(str)\n",
    "gk_players_link = 'https://www.transfermarkt.com' + pd.read_csv(PATH_TO_NOT_GK_PLAYERS_LINK)\n",
    "\n",
    "for i in range(len(gk_players_link)):\n",
    "    id = gk_players_link.loc[i, \"0\"].split('spieler/')[-1]\n",
    "    name = gk_players_link.loc[i, \"0\"].split('com/')[-1].split('/profil')[0].replace(' ', '-')\n",
    "    gk_players_hyperlink = f\"https://www.transfermarkt.com/{name}/leistungsdatendetails/spieler/{id}/plus/1?saison=2024&verein=&liga=&wettbewerb=&pos=&trainer_id=\"\n",
    "    single_gk_player_stat = scraping_gk_stat(gk_players_hyperlink, name)\n",
    "    gk_players_stat = pd.concat([gk_players_stat, pd.DataFrame([single_gk_player_stat])], ignore_index=True)\n",
    "\n",
    "pd.DataFrame(gk_players_stat).to_csv(PATH_TO_GK_PLAYERS_STAT, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01da2a3f",
   "metadata": {},
   "source": [
    "#### 5. PRE DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b4533a",
   "metadata": {},
   "source": [
    "Preparing to concatenate csv file *data* and csv file *stat* together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd55b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_gk_players_data = pd.read_csv(PATH_TO_NOT_GK_PLAYERS_DATA)\n",
    "gk_players_data = pd.read_csv(PATH_TO_GK_PLAYERS_DATA)\n",
    "not_gk_players_stat = pd.read_csv(PATH_TO_NOT_GK_PLAYERS_STAT)\n",
    "gk_players_stat = pd.read_csv(PATH_TO_GK_PLAYERS_STAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dda740",
   "metadata": {},
   "source": [
    "Adding the \"index\" column into every Dataframe serving as the mutual features to concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce58c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_gk_players_data['Index'] = range(1, len(not_gk_players_data) + 1)\n",
    "cols = ['Index'] + [col for col in not_gk_players_data.columns if col != 'Index']\n",
    "not_gk_players_data.to_csv(PATH_TO_NOT_GK_PLAYERS_DATA, index=False)\n",
    "\n",
    "gk_players_data['Index'] = range(1, len(gk_players_data) + 1)\n",
    "cols = ['Index'] + [col for col in gk_players_data.columns if col != 'Index']\n",
    "gk_players_data.to_csv(PATH_TO_GK_PLAYERS_DATA, index=False)\n",
    "\n",
    "not_gk_players_stat['Index'] = range(1, len(not_gk_players_stat) + 1)\n",
    "cols = ['Index'] + [col for col in not_gk_players_stat.columns if col != 'Index']\n",
    "not_gk_players_stat.to_csv(PATH_TO_NOT_GK_PLAYERS_STAT, index=False)\n",
    "\n",
    "gk_players_stat['Index'] = range(1, len(gk_players_stat) + 1)\n",
    "cols = ['Index'] + [col for col in gk_players_stat.columns if col != 'Index']\n",
    "gk_players_stat.to_csv(PATH_TO_GK_PLAYERS_STAT, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b08edb5",
   "metadata": {},
   "source": [
    "Concatenating csv file *data* and csv file *stat* together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c07707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_gk_players = pd.merge(not_gk_players_data, not_gk_players_stat, on='Index')\n",
    "gk_players = pd.merge(gk_players_data, gk_players_stat, on='Index')\n",
    "\n",
    "not_gk_players.to_csv(PATH_TO_NOT_GK_PLAYERS, index=False)\n",
    "gk_players.to_csv(PATH_TO_GK_PLAYERS, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da51f9c",
   "metadata": {},
   "source": [
    "Reading the two newly created csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_gk_players = pd.read_csv(PATH_TO_NOT_GK_PLAYERS)\n",
    "gk_players = pd.read_csv(PATH_TO_GK_PLAYERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8b99f6",
   "metadata": {},
   "source": [
    "Adjusting columns in the Dataframe *not_gk_players* so that it has the format with the Dataframe *gk_players*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcb8634",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_gk_players['goalkeeper_or_not'] = not_gk_players['position'].apply(lambda x: '1' if x == 'Goalkeeper' else '0')\n",
    "not_gk_players['goals_conceded'] = float(0)\n",
    "not_gk_players['clean_sheet'] = float(0)\n",
    "cols = list(not_gk_players.columns)\n",
    "position_index_4 = cols.index('position')\n",
    "position_index_5 = cols.index('red_cards')\n",
    "cols.insert(position_index_4 + 1, cols.pop(cols.index('goalkeeper_or_not')))\n",
    "cols.insert(position_index_5 + 1, cols.pop(cols.index('goals_conceded')))\n",
    "cols.insert(position_index_5 + 2, cols.pop(cols.index('clean_sheet')))\n",
    "not_gk_players = not_gk_players[cols]\n",
    "not_gk_players.head(1)\n",
    "not_gk_players.to_csv(PATH_TO_NOT_GK_PLAYERS)\n",
    "not_gk_players = pd.read_csv(PATH_TO_NOT_GK_PLAYERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bfda10",
   "metadata": {},
   "source": [
    "Adjusting columns in the Dataframe *gk_players* so that it has the format with the Dataframe *not_gk_players*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f97f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "gk_players['goalkeeper_or_not'] = gk_players['position'].apply(lambda x: '1' if x == 'Goalkeeper' else '0')\n",
    "gk_players['assists'] = float(0)\n",
    "gk_players['penalty_goals'] = float(0)\n",
    "gk_players['minutes_per_goal'] = float(0)\n",
    "cols = list(gk_players.columns)\n",
    "position_index_1 = cols.index('position')\n",
    "position_index_2 = cols.index('goals')\n",
    "position_index_3 = cols.index('clean_sheet')\n",
    "cols.insert(position_index_1 + 1, cols.pop(cols.index('goalkeeper_or_not')))\n",
    "cols.insert(position_index_2 + 1, cols.pop(cols.index('assists')))\n",
    "cols.insert(position_index_3 + 1, cols.pop(cols.index('penalty_goals')))\n",
    "cols.insert(position_index_3 + 2, cols.pop(cols.index('minutes_per_goal')))\n",
    "gk_players = gk_players[cols]\n",
    "gk_players.head(1)\n",
    "gk_players.to_csv(PATH_TO_GK_PLAYERS)\n",
    "gk_players = pd.read_csv(PATH_TO_GK_PLAYERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b29683",
   "metadata": {},
   "source": [
    "Concatenating two dataframe *not_gk_players* and *gk_players* together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d415ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.concat([not_gk_players, gk_players])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14ff3b0",
   "metadata": {},
   "source": [
    "Some useful function you may need while working with dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c9d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate two dataframe\n",
    "\"\"\" merged_dataframe = pd.concat([dataframe_a, dataframe_b]) \"\"\"\n",
    "\n",
    "\n",
    "# dropping columns of a dataframe\n",
    "\"\"\" dropping_column_dataframe = dataframe.drop(columns=['name_of_column_1', 'name_of_column_2']) \"\"\"\n",
    "\n",
    "\n",
    "# swapping two columns of a dataframe\n",
    "\"\"\"columns = list(dataframe.columns)                                                                \n",
    "\n",
    "name_of_column_1_idx = columns.index('name_of_column_1')\n",
    "name_of_column_2_idx = columns.index('name_of_column_2')\n",
    "columns[name_of_column_2_idx], columns[name_of_column_1_idx] = columns[name_of_column_1_idx], columns[name_of_column_2_idx]\n",
    "\n",
    "dataframe = dataframe[columns]\"\"\"\n",
    "\n",
    "\n",
    "# returning all columns' name\n",
    "\"\"\"column_names = dataframe.columns\"\"\"\n",
    "\n",
    "\n",
    "# returning a column's index\n",
    "\"\"\"column_index = dataframe.columns.get_loc('name_of_the_column')\"\"\"\n",
    "\n",
    "\n",
    "# splitting csv file into a number of small csv files\n",
    "\"\"\"dataframe = pd.read_csv('file's adress.csv')\n",
    "\n",
    "number_of_rows = len(dataframe)\n",
    "number_of_row_per_files = number_of_rows // number_of_small_files\n",
    "\n",
    "for i in range(number_of_small_files):\n",
    "    start_index = i * number_of_row_per_files\n",
    "    end_index = (i + 1) * number_of_row_per_files if i < number_of_small_files else number_of_rows\n",
    "    dataframe_part = dataframe.iloc[start_index:end_index]\n",
    "    dataframe_part.to_csv(f'file_{i + 1}.csv', index=False)\"\"\"\n",
    "\n",
    "\n",
    "# merging csv files into one csv file\n",
    "\"\"\"dataframe_1 = pd.read_csv('dataframe_1's adress.csv')\n",
    "dataframe_2 = pd.read_csv('dataframe_2's adress.csv')\n",
    "\n",
    "merged_df = pd.merge(dataframe_1, dataframe_2, on = 'name_of_the_mutual_column')\n",
    "\n",
    "merged_df.to_csv('merged_file.csv', index=False)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
